---
title: 消息队列
tags: 消息队列 MQ
sidebar:
  nav: docs-zh
---

### 为什么使用消息队列，消息队列都有哪些场景

#### 消息队列的使用场景主要有三个，即解耦、异步和削峰
解耦：

原始场景：A系统发送个数据到BCD三个系统，接口调用发送，那如果E系统也要这个数据呢？那如果C系统现在不需要了呢？现在A系统又要发送第二种数据了呢？A系统负责人濒临崩溃中。。。

使用MQ解耦后：系统A产生一条数据，发送到MQ中去，B、C、D系统需要这个数据就去MQ消费即可，此时新增系统E也需要A的数据，则也直接去MQ中消费；假如系统C不需要数据就取消对MQ消息的消费即可，系统A不需要考虑给谁发送数据，不需要考虑其他系统是否调用成功、失败超时等问题。

异步：

原始场景：A系统接收一个请求，需要在自己本地写库，还需要在BCD三个系统写库，自己本地写库要20ms，BCD三个系统分别写库要300ms、450ms、200ms。最终请求总延时是20 + 300 + 450 + 200 = 970ms，接近1s，用户体验极差。

使用MQ异步后：A系统接收请求，本系统写库花费20ms，之后发送三条消息到三个消息队列，花费5ms，系统BCD去自己对应的队列去消费消息，分别写库要300ms、450ms、200ms。对于用户而言响应时间是20+5=25ms。

削峰：

原始场景：每天0点到11点，A系统风平浪静，每秒并发请求数量就100个。结果每次一到12点~1点，每秒并发请求数量突然会暴增到5000条。但是系统最大的处理能力就只能是每秒钟处理2000个请求。

使用MQ削峰后：高峰期每秒的5000请求写入队列，系统A最多每秒处理2000请求，则每秒从队列中拉取2000请求，此时会导致大量请求积压在队列中，这个积压随着高峰期过去，系统A会消费掉。

#### 消息队列的缺点

由于引入的外部依赖增多导致系统可用性降低；使系统复杂性增高，需要考虑消息是否被重复消费，消息丢失的情况；还存在一致性的问题

### ActiveMQ、RabbitMQ、RocketMQ、Kafka对比

#### ActiveMQ

非常成熟，功能强大，在业内大量的公司以及项目中都有应用

偶尔会有较低概率丢失消息

而且现在社区以及国内应用都越来越少，官方社区现在对ActiveMQ 5.x维护越来越少，几个月才发布一个版本

而且确实主要是基于解耦和异步来用的，较少在大规模吞吐的场景中使用

#### RabbitMQ

erlang语言开发，性能极其好，延时很低；

吞吐量到万级，MQ功能比较完备

而且开源提供的管理界面非常棒，用起来很好用

社区相对比较活跃，几乎每个月都发布几个版本分

但是问题也是显而易见的，RabbitMQ确实吞吐量会低一些，这是因为他做的实现机制比较重。

而且erlang开发，如果对这个东西的掌控很弱，基本职能依赖于开源社区的快速维护和修复bug。

而且rabbitmq集群动态扩展会很麻烦，其实主要是erlang语言本身带来的问题。很难读源码，很难定制和掌控。

#### RocketMQ

接口简单易用，在阿里大规模应用过

日处理消息上百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是ok的，还可以支撑大规模的topic数量，支持复杂MQ业务场景

接口这块不是按照标准JMS规范走的有些系统要迁移需要修改大量代码

#### Kafka

仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展

同时kafka最好是支撑较少的topic数量即可，保证其超高吞吐量

kafka唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略

这个特性天然适合大数据实时计算以及日志收集

### 消息队列的高可用

#### RabbitMQ普通集群模式

就是在多台机器上启动多个rabbitmq实例，每个机器启动一个。但是创建的queue，只会放在一个rabbtimq实例上，但是每个实例都同步queue的元数据。消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从queue所在实例上拉取数据过来。

这种方式就是个普通集群。因为这导致要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个queue所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。

而且如果放queue的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果开启了消息持久化，让rabbitmq落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个queue拉取数据。

这种模式没有什么所谓的高可用性可言，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作。

#### RabbitMQ镜像集群模式

这种模式是所谓的rabbitmq的高可用模式，跟普通集群模式不一样的是，创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，然后每次写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。

这样的好处在于，任何一个机器宕机了，别的机器都可以用。坏处在于，第一，这个性能开销也太大，消息同步所有机器，导致网络带宽压力和消耗很重！第二，这样没有扩展性可言，如果某个queue负载很重，新增的机器也包含了这个queue的所有数据，并没有办法线性扩展queue

那么怎么开启这个镜像集群模式呢？其实很简单，rabbitmq有很好的管理控制台，在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候可以要求数据同步到所有节点的，也可以要求就同步到指定数量的节点，然后你再次创建queue的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。

### 如何保证消息不被重复消费（如何保证消息消费时的幂等性）

#### 产生问题的原因

kafka有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息的offset提交一下，代表我已经消费过了，下次要是重启，就继续从上次消费到的offset来继续消费。

offset是定时定期提交，如果重启时consumer有些消息处理了，但是没来得及提交offset，重启之后，少数消息会再次消费一次。

#### 解决办法

具体问题具体分析，比如消费消息时需要写库，先根据主键查一下，如果这数据都有了，就不插入；

可以在数据库设置唯一键约束；

让生产者发送每条数据的时候，里面加一个全局唯一的id，然后消费到了之后，先根据这个id去比如redis里查一下，如果没有消费过，你就处理，然后这个id写redis。如果消费过了，那就不处理，保证别重复处理相同的消息即可。

### 如何保证数据不丢失(RabbitMQ)

#### 生产者端

1、事务机制

生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，rabbitmq事务机制一搞，基本上吞吐量会下来，因为太耗性能。

2、Confirm模式

生产者将信道channel 设置成confirm模式，每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给回传一个ack消息，告诉这个消息ok了。如果rabbitmq没能处理这个消息，会回调一个nack接口，告诉这个消息接收失败，可以重试。

事务机制和cnofirm机制最大的不同在于，事务机制是同步的，提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调一个接口通知这个消息接收到了。

#### MQ端

开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小

设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。

#### 消费者端

用rabbitmq提供的ack机制，就是关闭rabbitmq自动ack，可以通过一个api来调用就行，然后每次代码里确保处理完的时候，再程序里ack一把。这样的话，如果还没处理完，不就没有ack？那rabbitmq就认为还没处理完，这个时候rabbitmq会把这个消费分配给别的consumer去处理，消息是不会丢的。

### 如何保证消息的顺序性

问题产生：

生产者产生三条需要被顺序消费的数据发送到队列，此时被三个消费者消费，此时无法保证三条顺序的消息分别被顺序消费。

解决：

将生产者产生三条需要被顺序消费的数据发送到队列后只由一个消费者去消费。即拆分多个queue，每个queue一个consumer。


